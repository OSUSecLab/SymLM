Namespace(align_suffix=None, alignfile=None, all_gather_list_size=16384, bf16=False, bpe=None, checkpoint_suffix='', cpu=False, criterion='cross_entropy', dataset_impl='mmap', destdir='../data_bin/self/byte3', empty_cache_freq=0, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, joined_dictionary=False, log_format=None, log_interval=100, lr_scheduler='fixed', memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_progress_bar=False, nwordssrc=-1, nwordstgt=-1, only_source=True, optimizer=None, padding_factor=8, profile=False, quantization_config_path=None, scoring='bleu', seed=None, source_lang=None, srcdict='vocabulary/byte3/dict.txt', target_lang=None, task='translation', tensorboard_logdir='', testpref='dataset_sample/test/self/input.byte3', tgtdict=None, threshold_loss_scale=None, thresholdsrc=0, thresholdtgt=0, tokenizer=None, tpu=False, trainpref='dataset_sample/train/self/input.byte3', user_dir=None, validpref='dataset_sample/valid/self/input.byte3', workers=40)
[None] Dictionary: 264 types
[None] dataset_sample/train/self/input.byte3: 43436 sents, 7528221 tokens, 0.0% replaced by <unk>
[None] Dictionary: 264 types
[None] dataset_sample/valid/self/input.byte3: 5043 sents, 857916 tokens, 0.0% replaced by <unk>
[None] Dictionary: 264 types
[None] dataset_sample/test/self/input.byte3: 10954 sents, 1360359 tokens, 0.0% replaced by <unk>
Wrote preprocessed data to ../data_bin/self/byte3
Namespace(align_suffix=None, alignfile=None, all_gather_list_size=16384, bf16=False, bpe=None, checkpoint_suffix='', cpu=False, criterion='cross_entropy', dataset_impl='mmap', destdir='../data_bin/self/byte3', empty_cache_freq=0, fp16=False, fp16_init_scale=128, fp16_no_flatten_grads=False, fp16_scale_tolerance=0.0, fp16_scale_window=None, joined_dictionary=False, log_format=None, log_interval=100, lr_scheduler='fixed', memory_efficient_bf16=False, memory_efficient_fp16=False, min_loss_scale=0.0001, model_parallel_size=1, no_progress_bar=False, nwordssrc=-1, nwordstgt=-1, only_source=True, optimizer=None, padding_factor=8, profile=False, quantization_config_path=None, scoring='bleu', seed=None, source_lang=None, srcdict='vocabulary/byte3/dict.txt', target_lang=None, task='translation', tensorboard_logdir='', testpref='dataset_sample/test/self/input.byte3', tgtdict=None, threshold_loss_scale=None, thresholdsrc=0, thresholdtgt=0, tokenizer=None, tpu=False, trainpref='dataset_sample/train/self/input.byte3', user_dir=None, validpref='dataset_sample/valid/self/input.byte3', workers=40)
[None] Dictionary: 264 types
[None] dataset_sample/train/self/input.byte3: 43436 sents, 7528221 tokens, 0.0% replaced by <unk>
[None] Dictionary: 264 types
[None] dataset_sample/valid/self/input.byte3: 5043 sents, 857916 tokens, 0.0% replaced by <unk>
[None] Dictionary: 264 types
[None] dataset_sample/test/self/input.byte3: 10954 sents, 1360359 tokens, 0.0% replaced by <unk>
Wrote preprocessed data to ../data_bin/self/byte3
